{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install selenium\n",
    "# !pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import requests\n",
    "from time import sleep\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import shutil # For image download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "webDriverLocation = 'C:/Users/rohit/Desktop/IIMa/chromedriver_win32/chromedriver.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [\"Url\", \"Category\",\"Position\", \"Title\", \"Location\", \"Organiser\",\"Amount_Raised\", \"Goal\", \"Duration\", \"Text\", \"Image\"]\n",
    "%store headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_category_URLs():\n",
    "    url = 'https://www.gofundme.com/discover'\n",
    "    driver = webdriver.Chrome(webDriverLocation)\n",
    "    driver.get(url)\n",
    "    \n",
    "    for elem in driver.find_elements_by_link_text('Show all categories'):\n",
    "        try:\n",
    "            elem.click()\n",
    "            print('Success click')\n",
    "        except:\n",
    "            print('FAILED click')\n",
    "            \n",
    "    source = driver.page_source\n",
    "\n",
    "    driver.close()\n",
    "\n",
    "    soup = BeautifulSoup(source, 'lxml')\n",
    "    cat_containers = soup.findAll(\"div\", {\"class\": \"section-categories grid-x small-up-2 medium-up-3 large-up-6\"})\n",
    "    categories = cat_containers[0].text #contains category names for this section\n",
    "    categories = categories.splitlines()\n",
    "    categories = list(filter(None, categories))\n",
    "\n",
    "    more_containers = soup.findAll(\"div\", {\"class\": \"section-categories grid-x small-up-2 medium-up-3 large-up-6 js-more-categories\"})\n",
    "    more_cats = more_containers[0].text\n",
    "    more_cats = more_cats.splitlines()\n",
    "    more_cats = list(filter(None, more_cats))\n",
    "    all_cats = categories + more_cats # should be len(all_cats) == 18\n",
    "    \n",
    "    return all_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_urls_from_categories(url, MoreGFMclicks = 5):\n",
    "    # eg. url = 'https://www.gofundme.com/discover/medical-fundraiser'\n",
    "    driver = webdriver.Chrome(webDriverLocation)\n",
    "    category = url.split(\"/\")[-1]\n",
    "    driver.get(url)\n",
    "    \n",
    "    for i in range(MoreGFMclicks):\n",
    "        if(len(driver.find_elements_by_link_text('Show More'))==0):\n",
    "            break\n",
    "        for elem in driver.find_elements_by_link_text('Show More'):\n",
    "            try:\n",
    "                elem.click()\n",
    "                print(category+ ': Succesful click %s' %(i+1))\n",
    "                \n",
    "            except:\n",
    "                print(category+ ': Unsuccesful click %s' %(i+1))\n",
    "                \n",
    "            sleep(10) #longer delay - more succesful\n",
    "            # 5 seconds to ensure GFM doesn't ban us\n",
    "            \n",
    "    source = driver.page_source\n",
    "        \n",
    "    driver.close()\n",
    "    \n",
    "    soup = BeautifulSoup(source, 'lxml')\n",
    "    \n",
    "    filename = \"data/webpages/{cat}_{date}.html\".format(cat = category, date = datetime.datetime.now().strftime(\"%x\").replace(\"/\",\"_\") )\n",
    "    with open(filename, \"w\", encoding='utf-8') as file:\n",
    "        file.write(str(soup))\n",
    "    \n",
    "    containers = soup.findAll(\"div\", {\"class\": \"cell grid-item small-6 medium-4 js-fund-tile\"})\n",
    "    \n",
    "    print(\"Number of {0} Fundraisers found:\".format(category)+str(len(containers)))\n",
    "    \n",
    "    temp_url = {}\n",
    "    i = 1\n",
    "    for container in containers:\n",
    "        for link in container.find_all('a'):\n",
    "            temp_url[link.get('href')] = i\n",
    "            i = i + 1 \n",
    "\n",
    "    temp_url = {k: ((v // 2) - 1) // 3  for k, v in temp_url.items()} #\n",
    "\n",
    "\n",
    "    return(temp_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Url',\n",
       " 'Category',\n",
       " 'Position',\n",
       " 'Title',\n",
       " 'Location',\n",
       " 'Organiser',\n",
       " 'Amount_Raised',\n",
       " 'Goal',\n",
       " 'Duration',\n",
       " 'Text',\n",
       " 'Image']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstScrapingError = []\n",
    "\n",
    "def fetchFailedScrapes():\n",
    "    return lstScrapingError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def null(inp):\n",
    "    return \"\" if not inp else inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_url(url, tid = \"mainThread\"):\n",
    "    print(\"Scraping url:\"+url)\n",
    "    \n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'lxml')\n",
    "    errorMessage = \"\"\n",
    "    \n",
    "    scrapedDocument = {\"WebPage\" : page.text}\n",
    "\n",
    "    try:\n",
    "        # Description\n",
    "        text_container =  soup.find('meta', attrs={'name': 'description'})\n",
    "        try:\n",
    "            all_text = text_container['content']\n",
    "            if((all_text is not np.nan )or\n",
    "               (all_text != \"We're sorry, but that campaign cannot be found. Please check the link URL and try again.\")):\n",
    "                scrapedDocument[\"Description\"] = all_text\n",
    "            else:\n",
    "                errorMessage = errorMessage+ \"Invalid Campaign\"\n",
    "                lstScrapingError.append({url: \"Invalid Campaign\"})\n",
    "                return {}\n",
    "        except Exception as e:\n",
    "            print(url+\" Scraping error: \"+str(e))\n",
    "            errorMessage = errorMessage+ \"\\n Scraping error: \"+str(e)\n",
    "\n",
    "        # Title\n",
    "        try:\n",
    "            scrapedDocument[\"Title\"] = soup.find(\"title\").text\n",
    "        except Exception as e:\n",
    "            errorMessage = errorMessage+ \"\\n Title error: \"+str(e)\n",
    "\n",
    "        # Organiser & Location\n",
    "        try:\n",
    "            t = soup.find_all(\"div\",{\"class\":\"p-campaign-members\"})[0]\n",
    "            scrapedDocument[\"Organiser\"] = t.find_all(\"div\",{\"class\": \"m-person-info-name\"})[0].text.replace('\\xa0','')\n",
    "            location = \"\"\n",
    "            if(len(t.find_all(\"div\",{\"class\": \"text-small\"})) > 3):\n",
    "                location = t.find_all(\"div\",{\"class\": \"text-small\"})[2].text.replace('\\xa0','')\n",
    "            elif(len(t.find_all(\"div\",{\"class\": \"text-small\"})) == 1):\n",
    "                location = \"\"\n",
    "            else:\n",
    "                location = t.find_all(\"div\",{\"class\": \"text-small\"})[1].text.replace('\\xa0','')\n",
    "            scrapedDocument[\"Location\"] = location\n",
    "        except Exception as e:\n",
    "            errorMessage = errorMessage+ \"\\n Location and organiser error: \"+str(e)\n",
    "\n",
    "        # Amount Raised and Goal\n",
    "        try:\n",
    "            t = soup.find(\"div\",{\"class\":\"p-campaign-sidebar\"})\n",
    "            amount = t.find(\"div\",{\"class\": \"o-campaign-sidebar-progress-meter m-progress-meter\"}).text\n",
    "            scrapedDocument[\"Amount_Raised\"] = int(amount.split()[0][1:].replace(\",\",\"\"))\n",
    "            goal = 0\n",
    "            if(len(amount.split()) > 2):                \n",
    "                goal = int(amount.split()[-2][1:].replace(\",\",\"\"))\n",
    "            scrapedDocument[\"Goal\"] = goal\n",
    "        except Exception as e:\n",
    "            errorMessage = errorMessage+ \"\\n goal and Amount error: \"+str(e)\n",
    "\n",
    "        # All Text\n",
    "        soup.find('div', {'class': \"p-campaign-description\"})\n",
    "        scrapedDocument[\"Text\"] = soup.find('div', {'class': \"o-campaign-story mt-three-halves\"}).text.replace('\\xa0','').replace('\\n','')\n",
    "\n",
    "        # Duration of Fundraising\n",
    "        try:\n",
    "            t = soup.find(\"div\",{\"class\": \"p-campaign-description\"})\n",
    "            startDate = ' '.join(t.find(\"li\",{\"class\": \"m-meta-list-item\"}).text.split()[1::])\n",
    "            startDate = (pd.to_datetime(startDate)).date()\n",
    "            scrapedDocument[\"Duration\"] = (pd.datetime.now().date() - startDate).days\n",
    "        except Exception as e:\n",
    "            errorMessage = errorMessage+ \"\\n duration error: \"+str(e)\n",
    "        \n",
    "        #Image\n",
    "        try:\n",
    "            img_container =  soup.find('meta', attrs={'property': 'og:image'})\n",
    "            image_url = img_container['content']\n",
    "\n",
    "            img_filename = \"data/images/\"+image_url.split(\"/\")[-1]\n",
    "\n",
    "            # Open the url image, set stream to True, this will return the stream content.\n",
    "            r = requests.get(image_url, stream = True)\n",
    "\n",
    "            # Check if the image was retrieved successfully\n",
    "            if r.status_code == 200:\n",
    "                # Set decode_content value to True, otherwise the downloaded image file's size will be zero.\n",
    "                r.raw.decode_content = True\n",
    "                scrapedDocument[\"Image\"] = r.raw.read()\n",
    "        except Exception as e:\n",
    "            errorMessage = errorMessage+ \"\\n goal and Amount error: \"+str(e)\n",
    "            \n",
    "        scrapedDocument[\"ErrorMessage\"] = errorMessage\n",
    "            \n",
    "        return scrapedDocument\n",
    "    except Exception as e:\n",
    "        lstScrapingError.append({url: str(e)})\n",
    "        return {}\n",
    "\n",
    "    #     # FB likes and Shares    \n",
    "    #     try:\n",
    "    #         FB_shares_container = soup.find_all(\"strong\", {\"class\":\"js-share-count-text\"})\n",
    "    #         FB_shares = FB_shares_container[0].text.splitlines()\n",
    "    #         FB_shares = FB_shares[1].replace(\" \", \"\").replace(\"\\xa0\", \"\")\n",
    "    #     except:\n",
    "    #         FB_shares = np.nan\n",
    "\n",
    "    #     try:\n",
    "    #         hearts_container = soup.find_all(\"div\", {\"class\":\"campaign-sp campaign-sp--heart fave-num\"})\n",
    "    #         hearts = hearts_container[0].text\n",
    "    #     except:\n",
    "    #         hearts = np.nan\n",
    "\n",
    "#         temp_row = np.array([[url, category, position, title, location, organiser, amountRaised, goal, duration, text, img_filename]])\n",
    "#         temp_df = pd.DataFrame(temp_row, columns = headers)\n",
    "\n",
    "#         return(temp_df)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping url:https://www.gofundme.com/f/collin-kartchner-memorial-fund\n",
      "duration error Handling'NoneType' object has no attribute 'text'\n"
     ]
    }
   ],
   "source": [
    "# x = scrape_url(\"https://www.gofundme.com/f/collin-kartchner-memorial-fund\",\"Trial Thread\")\n",
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showImage(image_data):\n",
    "    img_array = bytearray(image_data)\n",
    "    image = Image.open(io.BytesIO(img_array))\n",
    "    image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showImage(x[\"Image\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scrape_url_prev(row_index, tid = \"mainThread\"):\n",
    "#     print(\"Scraping url %s\" %(row_index+1))\n",
    "    \n",
    "# #     single_row = mydf.iloc[row_index]\n",
    "#     url = row[\"Url\"]\n",
    "#     category = row[\"Category\"]\n",
    "#     position = row[\"Position\"]\n",
    "\n",
    "#     page = requests.get(url)\n",
    "#     soup = BeautifulSoup(page.text, 'lxml')\n",
    "    \n",
    "# #     temp_df = pd.DataFrame(columns = headers)\n",
    "    \n",
    "# #     invalidCampaign = soup.find('div', attrs={'class': 'm-campaign-sidebar-notification m-well m-well--sand'})\n",
    "    \n",
    "# #     if(invalidCampaign is not None):\n",
    "# #         text_container =  soup.find('meta', attrs={'name': 'description'})\n",
    "# #         print(text_container['content'])\n",
    "# #         listInvalidCampaign.append(row_index)\n",
    "# #         return \"Invalid Campaign\"\n",
    "    \n",
    "#     try:\n",
    "#         # Description\n",
    "#         text_container =  soup.find('meta', attrs={'name': 'description'})\n",
    "#         try:\n",
    "#             all_text = text_container['content']\n",
    "#         except:\n",
    "#             all_text = np.nan\n",
    "\n",
    "#         if(all_text is np.nan )or(all_text == \"We're sorry, but that campaign cannot be found. Please check the link URL and try again.\"):\n",
    "#             print(tid+' ('+str(row_index)+') -> '+all_text)\n",
    "#             listInvalidCampaign.append(row_index)\n",
    "#             temp_df = pd.DataFrame(columns = headers)\n",
    "#             return(temp_df)\n",
    "\n",
    "#         # Title\n",
    "#         try:\n",
    "#             title = soup.find(\"title\").text\n",
    "#         except:\n",
    "#             title = np.nan\n",
    "\n",
    "#         # Organiser & Location\n",
    "#         try:\n",
    "#             t = soup.find_all(\"div\",{\"class\":\"p-campaign-members\"})[0]\n",
    "#             organiser = t.find_all(\"div\",{\"class\": \"m-person-info-name\"})[0].text.replace('\\xa0','')\n",
    "#             location = \"\"\n",
    "#             if(len(t.find_all(\"div\",{\"class\": \"text-small\"})) > 3):\n",
    "#                 location = t.find_all(\"div\",{\"class\": \"text-small\"})[2].text.replace('\\xa0','')\n",
    "#             else:\n",
    "#                 location = t.find_all(\"div\",{\"class\": \"text-small\"})[1].text.replace('\\xa0','')\n",
    "#         except:\n",
    "#             organiser = np.nan\n",
    "#             location = np.nan\n",
    "\n",
    "#         # Amount Raised and Goal\n",
    "#         try:\n",
    "#             t = soup.find(\"div\",{\"class\":\"p-campaign-sidebar\"})\n",
    "#             amount = t.find(\"div\",{\"class\": \"o-campaign-sidebar-progress-meter m-progress-meter\"})\n",
    "#             amountRaised = amount.text.split()[0]\n",
    "#             goal = amount.text.split()[-2]\n",
    "#         except:\n",
    "#             amountRaised = np.nan\n",
    "#             goal = np.nan\n",
    "\n",
    "#         # All Text\n",
    "#         soup.find('div', {'class': \"p-campaign-description\"})\n",
    "#         text = soup.find('div', {'class': \"o-campaign-story mt-three-halves\"}).text.replace('\\xa0','').replace('\\n','')\n",
    "#         text\n",
    "\n",
    "#         # Duration of Fundraising\n",
    "#         try:\n",
    "#             t = soup.find(\"div\",{\"class\": \"p-campaign-description\"})\n",
    "#             startDate = ' '.join(t.find(\"li\",{\"class\": \"m-meta-list-item\"}).text.split()[1::])\n",
    "#             startDate = (pd.to_datetime(startDate)).date()\n",
    "#             duration = (pd.datetime.now().date() - startDate).days\n",
    "#         except:\n",
    "#             duration = np.nan\n",
    "\n",
    "#         #Image\n",
    "#         try:\n",
    "#             img_container =  soup.find('meta', attrs={'property': 'og:image'})\n",
    "#             image_url = img_container['content']\n",
    "\n",
    "#             img_filename = \"data/images/\"+image_url.split(\"/\")[-1]\n",
    "\n",
    "#             # Open the url image, set stream to True, this will return the stream content.\n",
    "#             r = requests.get(image_url, stream = True)\n",
    "\n",
    "#             # Check if the image was retrieved successfully\n",
    "#             if r.status_code == 200:\n",
    "#                 # Set decode_content value to True, otherwise the downloaded image file's size will be zero.\n",
    "#                 r.raw.decode_content = True\n",
    "\n",
    "#                 # Open a local file with wb ( write binary ) permission.\n",
    "#                 with open(img_filename,'wb') as f:\n",
    "#                     shutil.copyfileobj(r.raw, f)\n",
    "#         except:\n",
    "#             img_filename = np.nan\n",
    "\n",
    "#     #     # FB likes and Shares    \n",
    "#     #     try:\n",
    "#     #         FB_shares_container = soup.find_all(\"strong\", {\"class\":\"js-share-count-text\"})\n",
    "#     #         FB_shares = FB_shares_container[0].text.splitlines()\n",
    "#     #         FB_shares = FB_shares[1].replace(\" \", \"\").replace(\"\\xa0\", \"\")\n",
    "#     #     except:\n",
    "#     #         FB_shares = np.nan\n",
    "\n",
    "#     #     try:\n",
    "#     #         hearts_container = soup.find_all(\"div\", {\"class\":\"campaign-sp campaign-sp--heart fave-num\"})\n",
    "#     #         hearts = hearts_container[0].text\n",
    "#     #     except:\n",
    "#     #         hearts = np.nan\n",
    "\n",
    "#         temp_row = np.array([[url, category, position, title, location, organiser, amountRaised, goal, duration, text, img_filename]])\n",
    "#         temp_df = pd.DataFrame(temp_row, columns = headers)\n",
    "\n",
    "#         return(temp_df)\n",
    "    \n",
    "#     except:\n",
    "#         lstScrapingError.append(row_index)\n",
    "#         return tid+' ('+str(row_index)+') -> '+\"Scraping Error\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
